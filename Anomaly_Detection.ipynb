{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Anomaly_Detection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YiVfKn-6tXz8"},"source":["# **Anomaly Detection**"]},{"cell_type":"markdown","metadata":{"id":"6qLcLuhETSDH"},"source":["# Load and preprocess data"]},{"cell_type":"code","metadata":{"id":"cBo2oxu_WmZY","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"29b688e0-f497-4323-c4ca-c3c75294c623"},"source":["!gdown --id '1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq' --output train.npy \n","!gdown --id '11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr' --output test.npy \n","\n","import numpy as np\n","\n","train = np.load('train.npy', allow_pickle=True)\n","test = np.load('test.npy', allow_pickle=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq\n","To: /content/train.npy\n","983MB [00:05, 172MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr\n","To: /content/test.npy\n","246MB [00:05, 47.0MB/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LrpPP6x3XHo_"},"source":["# Task"]},{"cell_type":"code","metadata":{"id":"QIHQqB7IXOXD"},"source":["task = 'knn'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4GwvK4DXuW4"},"source":["# KNN"]},{"cell_type":"code","metadata":{"id":"B7AB9wnaX1To","colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"46802b73-3f99-4606-d9d7-591e637b8ee6"},"source":["from sklearn.cluster import MiniBatchKMeans\n","from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score\n","from scipy.cluster.vq import vq, kmeans\n","\n","\n","if task == 'knn':\n","    x = train.reshape(len(train), -1) # 40000 * 32 * 32 * 3 => 40000 * 3072\n","    y = test.reshape(len(test), -1) # 10000 * 32 * 32 * 3 => 10000 * 3072\n","    scores = list()\n","    for n in range(1, 9): # 原為 10（總共10種數字，而訓練資料中最多有9種）\n","      kmeans_x = MiniBatchKMeans(n_clusters=n, batch_size=100).fit(x) # 先做 kmeans，並嘗試不同 n 的效果\n","      print(kmeans_x)\n","      y_cluster = kmeans_x.predict(y) # 再做 knn\n","      print(y_cluster)\n","      y_dist = np.sum(np.square(kmeans_x.cluster_centers_[y_cluster] - y), axis=1)\n","      y_pred = y_dist\n","    #   score = f1_score(y_label, y_pred, average='micro')\n","    #   score = roc_auc_score(y_label, y_pred, average='micro')\n","    #   scores.append(score)\n","    # print(np.max(scores), np.argmax(scores))\n","    # print(scores)\n","    # print('auc score: {}'.format(np.max(scores)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=1, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[0 0 0 ... 0 0 0]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=2, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[0 1 1 ... 0 0 0]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=3, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[2 1 0 ... 2 2 0]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=4, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[1 2 2 ... 0 1 1]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=5, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[0 1 1 ... 0 0 1]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=6, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[3 5 5 ... 4 2 3]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=7, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[1 5 5 ... 6 6 1]\n","MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n","                init_size=None, max_iter=100, max_no_improvement=10,\n","                n_clusters=8, n_init=3, random_state=None,\n","                reassignment_ratio=0.01, tol=0.0, verbose=0)\n","[6 2 2 ... 3 1 4]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QLScetmgf4XV"},"source":["# PCA"]},{"cell_type":"code","metadata":{"id":"EzgZGG7Pf6Qm"},"source":["from sklearn.decomposition import PCA\n","\n","if task == 'pca':\n","\n","    x = train.reshape(len(train), -1)\n","    y = test.reshape(len(test), -1)\n","    pca = PCA(n_components=2).fit(x)\n","\n","    y_projected = pca.transform(y)\n","    y_reconstructed = pca.inverse_transform(y_projected)  \n","    # print(np.square(y_reconstructed - y).shape)\n","    # print(np.square(y_reconstructed - y).reshape(len(y), -1).shape) # reshape 是多餘的\n","    dist = np.sqrt(np.sum(np.square(y_reconstructed - y).reshape(len(y), -1), axis=1))\n","    \n","    y_pred = dist\n","    # score = roc_auc_score(y_label, y_pred, average='micro')\n","    # score = f1_score(y_label, y_pred, average='micro')\n","    # print('auc score: {}'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zR9zC0_Df-CR"},"source":["# Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"1EbfwRREhA7c"},"source":["# Models & loss"]},{"cell_type":"code","metadata":{"id":"Wi8ds1fugCkR"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","class fcn_autoencoder(nn.Module):\n","    def __init__(self):\n","        super(fcn_autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(32 * 32 * 3, 128),\n","            nn.ReLU(True),\n","            nn.Linear(128, 64),\n","            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n","        self.decoder = nn.Sequential(\n","            nn.Linear(3, 12),\n","            nn.ReLU(True),\n","            nn.Linear(12, 64),\n","            nn.ReLU(True),\n","            nn.Linear(64, 128),\n","            nn.ReLU(True), nn.Linear(128, 32 * 32 * 3\n","            ), nn.Tanh())\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","class conv_autoencoder(nn.Module):\n","    def __init__(self):\n","        super(conv_autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16] => 一次動2格，補一個0\n","            nn.ReLU(),\n","            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n","            nn.ReLU(),\n","\t\t\t      nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n","            nn.ReLU(),\n","    # \t\t\tnn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n","    #       nn.ReLU(),\n","        )\n","        self.decoder = nn.Sequential(\n","#             nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n","#             nn.ReLU(),\n","            # output_size = strides * (input_size-1) + kernel_size - 2*padding\n","\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n","            nn.ReLU(),\n","\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# vae: https://blog.csdn.net/a312863063/article/details/87953517\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(32*32*3, 400)\n","        self.fc21 = nn.Linear(400, 20)\n","        self.fc22 = nn.Linear(400, 20)\n","        self.fc3 = nn.Linear(20, 400)\n","        self.fc4 = nn.Linear(400, 32*32*3)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparametrize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_() # mul() 使矩陣對應位相乘\n","        if torch.cuda.is_available():\n","            eps = torch.cuda.FloatTensor(std.size()).normal_() ### 需再確認用法\n","        else:\n","            eps = torch.FloatTensor(std.size()).normal_()\n","        eps = Variable(eps) # 用 variable 儲存\n","        return eps.mul(std).add_(mu)\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return F.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparametrize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","\n","def loss_vae(recon_x, x, mu, logvar, criterion):\n","    \"\"\"\n","    recon_x: generating images\n","    x: origin images\n","    mu: latent mean\n","    logvar: latent log variance\n","    \"\"\"\n","    mse = criterion(recon_x, x)  # mse loss\n","    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2) 將分配帶入得到的結果\n","    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar) # 有底線代表 inplace\n","    KLD = torch.sum(KLD_element).mul_(-0.5)\n","    # KL divergence\n","    return mse + KLD\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bCzvlZtXdXp"},"source":["task = 'ae'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKNUImqUhIeq"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"JoW1UrrxgI_U","colab":{"base_uri":"https://localhost:8080/","height":411},"outputId":"71481741-2e3f-4017-8160-c6e84b3f8724"},"source":["from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam, AdamW\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","\n","\n","if task == 'ae':\n","    num_epochs = 1000\n","    batch_size = 128\n","    learning_rate = 1e-3\n","\n","    #{'fcn', 'cnn', 'vae'} \n","    model_type = 'fcn' \n","\n","    x = train\n","    if model_type == 'fcn' or model_type == 'vae':\n","        x = x.reshape(len(x), -1)\n","        \n","    data = torch.tensor(x, dtype=torch.float)\n","    train_dataset = TensorDataset(data)\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n","\n","\n","    model_classes = {'fcn':fcn_autoencoder(), 'cnn':conv_autoencoder(), 'vae':VAE()}\n","    model = model_classes[model_type].cuda()\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(), lr=learning_rate)\n","    \n","    best_loss = np.inf\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for data in train_dataloader:\n","            if model_type == 'cnn':\n","                img = data[0].transpose(3, 1).cuda()\n","            else:\n","                img = data[0].cuda()\n","            # ===================forward=====================\n","            output = model(img)\n","            if model_type == 'vae':\n","                loss = loss_vae(output[0], img, output[1], output[2], criterion)\n","            else:\n","                loss = criterion(output, img)\n","            # ===================backward====================\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # ===================save====================\n","            if loss.item() < best_loss:\n","                best_loss = loss.item()\n","                torch.save(model, 'best_model_{}.pt'.format(model_type))\n","        # ===================log========================\n","        print('epoch [{}/{}], loss:{:.4f}'\n","              .format(epoch + 1, num_epochs, loss.item()))\n","        \n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type fcn_autoencoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["epoch [1/1000], loss:0.1247\n","epoch [2/1000], loss:0.1289\n","epoch [3/1000], loss:0.1392\n","epoch [4/1000], loss:0.1319\n","epoch [5/1000], loss:0.1262\n","epoch [6/1000], loss:0.1251\n","epoch [7/1000], loss:0.1177\n","epoch [8/1000], loss:0.1162\n","epoch [9/1000], loss:0.1366\n","epoch [10/1000], loss:0.1329\n","epoch [11/1000], loss:0.1093\n","epoch [12/1000], loss:0.1468\n","epoch [13/1000], loss:0.1304\n","epoch [14/1000], loss:0.1120\n","epoch [15/1000], loss:0.1161\n","epoch [16/1000], loss:0.1224\n","epoch [17/1000], loss:0.1295\n","epoch [18/1000], loss:0.1213\n","epoch [19/1000], loss:0.1258\n","epoch [20/1000], loss:0.1411\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wk0UxFuchLzR"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"r1PS_ApzhfOQ"},"source":["if task == 'ae':\n","    if model_type == 'fcn' or model_type == 'vae':\n","        y = test.reshape(len(test_tmp), -1)\n","    else:\n","        y = test\n","        \n","    data = torch.tensor(y, dtype=torch.float)\n","    test_dataset = TensorDataset(data)\n","    test_sampler = SequentialSampler(test_dataset)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n","\n","    model = torch.load('best_model_{}.pt'.format(model_type), map_location='cuda')\n","\n","    model.eval()\n","    reconstructed = list()\n","    for i, data in enumerate(test_dataloader): \n","        if model_type == 'cnn':\n","            img = data[0].transpose(3, 1).cuda() # numpy 和 tensor 儲存圖片的方式不同\n","        else:\n","            img = data[0].cuda()\n","        output = model(img)\n","        if model_type == 'cnn':\n","            output = output.transpose(3, 1) # 換回來\n","        elif model_type == 'vae':\n","            output = output[0]\n","        reconstructed.append(output.cpu().detach().numpy())\n","\n","    reconstructed = np.concatenate(reconstructed, axis=0)\n","    anomality = np.sqrt(np.sum(np.square(reconstructed - y).reshape(len(y), -1), axis=1))\n","    y_pred = anomality\n","    with open('prediction.csv', 'w') as f:\n","        f.write('id,anomaly\\n')\n","        for i in range(len(y_pred)):\n","            f.write('{},{}\\n'.format(i+1, y_pred[i]))\n","    # score = roc_auc_score(y_label, y_pred, average='micro')\n","    # score = f1_score(y_label, y_pred, average='micro')\n","    # print('auc score: {}'.format(score))\n"],"execution_count":null,"outputs":[]}]}